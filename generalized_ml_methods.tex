\section*{Methods for Regression and Classification}

\textbf{Support Vector Machines} (SVM) 
can be used for classification or continuous values for regression. 
The class of an unseen data point is determined by comparing it with the support vectors in the training set by using the chosen kernel.
SVMs rely on solving a convex optimization problem.

\textit{Kernels}
define a hyperplane in a higher dimensional feature space where the data is linearly separable. 
The mapping to the higher dimensional space is defined by $\phi(x)$. 
Kernel functions calculate the similarity of the two vectors without having to calculate the mapping function $k(x,x')=\phi(x^T)\phi(x')$.
Common kernel functions:
Linear: $k(x,x') = x^T x'$, polynomial: $k(x,x')=(x^T x' + 1)^p, p\in\doubleN$, Gaussian: $k(x,x')=\exp{(-\sfrac{\norm{x-x'}^2}{2\sigma^2})}$, radial basis function: $k(x,x')=\exp{-\gamma\norm{x-x'}^2}$ (hyperparameter $\gamma$ determines how much influence data points with lower similarity have on the decision boundary).

\textbf{Support Vector Classification} (SVC):
In two-class classification, the goal is to find a decision surface $y(x)=w^T\phi(x)+b$, with $w$: learning parameter vector, $\phi(x)$: feature transformation, $b$: bias.
The training set consists of input samples $x_1,...x_N$ with corresponding target values $t_1,...,t_N$ where $t_i\in {-1,1}$.
Under the assumption that the data set is linearly separable in the transformed feature space $\phi(x)$, at least one solution $w,b$ exists such that $t_n y(x_n)>0$ for all data points.
SVMs find the single decision boundary $y(x)$ that maximizes the smallest distance between any sample to itself (also called the margin).

SVC is helpful, as the class of an unseen data point is determined by comparing it to the support vectors in the training set by using the chosen kernel.
The kernel trick allows for the insertion of an arbitrary kernel that corresponds to an indefinitely dimensional feature transformation (e.g., the Gaussian kernel), but without calculating the feature transformation.

\textit{Hard Margin SVC}: can lack from overfitting to the training data. In general, a perfect classification!
However, the resulting decision surface will give a poor generalization.

\textit{Soft Margin SVC}
A soft-margin SVC counteracts overfitting.
The approach of the hard-margin SVC is modified such that data points are allowed on the wrong side of the margin.
Slack variables $\xi_n$ are defined for every training data point: $\xi_n = \abs{t_n-y(x_n)}$.
A penalty is introduced that increases linearly with the distance from the boundary when a data point is misclassified.
With $C$ controlling trade-off between the slack variable penalty and the decision boundary: $\min(\frac{1}{2}\norm{w}^2)+C\sum_{n=1}^{N} \xi_n$, s.t. $t_n y(x_n) \geq 1 - \xi_n, n = 1,..., N$.

\textbf{Tipps \& Tricks}
Use FE results as training data for SVM classification.
Sample data points using Sliced Latin Hypercube Sampling. Filter out infeasible configurations.
Assess the performance of the SVM using Cross-Validation.

Hyperparameter optimization: Choose manually (difficult \& time consuming) or use an optimization process, e.g.:
\textit{SVM Optimization Equation with \textcolor{purple}{RBF Kernel}}: $\max_w (\Tilde{L}(w)) = \sum_{n=1}^N \lambda_n - \frac{1}{2} \sum_{n=1}^N \sum_{m=1}^N \lambda_n \lambda_m t_n t_m \textcolor{purple}{\exp{(-\gamma\norm{x_n-x_m}^2)}}$, with $\gamma$: kernel scale and $C$: box constraint, s.t. $0\leq \lambda_n \leq C, \sum_{n=1}^N \lambda_n t_n = 0$.

\textbf{K-Fold cross validation} 
separates the training data multiple times during the loss calculation to avoid introducing bias into the loss prediction.
There exist several classification loss equations.

\textbf{Kriging vs. SVM parameter determination}
Iterate until convergence. 
Kriging minimizes the cost function $L(\theta,sigma^2, a)$ for constant regression parameter $a$ to get $\theta, \sigma^2$. 
Thereafter, you minimize the cost function for constant correlation parameters $\theta$ and covariance $\sigma^2$ to get $a$. Start again.
For SVM, the cost function is $L(C, \gamma, w)$, first for constant $w$, then for constant hyper-parameters $C$ and $\gamma$.

\textbf{Support Vector Machines Regression} (SVR)
SVM not only does support linear and nonlinear classification, but it also supports linear and nonlinear regression.
Just reverse the objective: instead of trying to fit the largest possible street between two classes while limiting margin violations, SVM Regression tries to fit as many instances as possible on the street while limiting margin violations.
The width of the street is controlled by a hyperparameter $\epsilon$.
In SVR, the error function from Ridge Regression is replaced by a function that only penalizes data points that lie ouside the $\epsilon$-tube:

$E_\epsilon (t-y(x)) = \begin{cases} 
0 &\mbox{if } \abs{t-y(x)} < \epsilon \\
\abs{t-y(x)} - \epsilon & \mbox{otherwise}
\end{cases}$.
The regularized error function in SVR that needs to be minized is:
$\frac{1}{2} \norm{w}^2 - C \sum_{n=1}^N E_{\epsilon} (y(x_n) - t_n)$.
Note: $C$: Regularization parameter (hyper-parameter), $y(x_n)=w^T \phi(x_n)+b$.

Similar to SVC slack variables are introduced: $\xi$ and $\hat{\xi}$, per data point that needs to be optimized.
$\xi>0$ corresponds to a point that fulfils $t_n > y(x_n) + \epsilon$. 
$\hat{\xi}>0$ corresponds to a point that fulfils $t_n < y(x_n) - \epsilon$. 
The conditions on data points are: $t_n \leq y(x_n) + \epsilon + \xi_n$ and $t_n \geq y(x_n) - \epsilon - \hat{\xi}_n$.
Minimizing the slack variables leads to error function:
$\frac{1}{2} \norm{w}^2 - C \sum_{n=1}^N (\xi_n + \hat{\xi}_n)$, s.t. $\xi_n\geq 0, \hat{\xi}_n \geq 0$ and the said conditions on data points.

The dual formulation is found by substituting $y(x)$ and setting the derivatives of the Lagrangian to zero to be:
$max_{a_n, \hat{a}_n} \tilde{L} = \frac{1}{2} \sum_{n=1}^N \sum_{m=1}^N (a_n - \hat{a}_n) (a_m - \hat{a}_m) \textcolor{purple}{k(x_n,x_m)} - \epsilon \sum_{n=1}^N (a_n - \hat{a}_n) + \sum_{n=1}^N (a_n - \hat{a}_n)t_n$, 
s.t. $0\leq a_n \leq C, 0 \leq \hat{a}_n \leq C$.
Predictions for new inputs are made by $y(x) = \sum_{n=1}^N (a_n - \hat{a}_n) \textcolor{purple}{k(x,x_n)} + b$.

\textbf{Pricipal Component Analyis (PCA)}
Sampling covariance matrix of $X$ ($d$ features as columns, $n$ observations in rows): $C_X = \sfrac{1}{n-1}X^T X$.
Symmetric. Variance of the columns of $X$ in diagonal and correlation in off-diagonals.
Goal of PCA: find a coordinate system where $C_x = P^T C_Z P$ is diagonal. 
Columns of $P$ contain the eigenvectors of $C_X$, $C_Z$ is the sampling covariance with respect to the new basis $P$.
The new basis vectors are referred to as principal component directions.
$C_Z$ is diagonal and contains the variances of $Z = XP$.
The columns of $Z$ are called principal components.
Since the off-diagonal elements of $C_Z$ are zero, the correlations of the principal components are zero.
The dimensionality of $Z$ can be reduced by neglecting the principal components, which contribute the fewest of all to the total variance in the data.

